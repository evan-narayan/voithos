# OpenStack Service Configuration

There are two ways to configure OpenStack's services.

The first and most common method is to modify Kolla-Ansible's [globals file](/openstack-kolla-globals.html).
While the options directly supported by Kolla-Ansible and its globals file are extensive, they
aren't exhaustive. Some configurations, such as Ceph's keyring files, can't be done this way.

The second approach involves writing your own configuration files. They'll be merged with those
generated by Kolla-Ansible and overwrite any values it had previously generated. The files are
expected to have particular names and exist in specific directories to function correctly. When
deploying a basic LVM cluster, no custom files need to be written.

To fine-tune the OpenStack services, create a `config/` directory.

This directory will be used with the `voithos openstack kolla-ansible` command,
specified by the `--config-dir` option.

Ceph-backed clusters should have already created some files here, as specified in the
[OpenStack Ceph setup guide](/openstack-ceph.html).


---


## config/ceilometer/polling.yaml

### Metric polling

This configuration is strictly required for the Arcus self-service portal.
The `polling.yaml` file defines the metrics required by the Arcus self-service portal.

```
# config/ceilometer/polling.yaml
---
sources:
    - name: some_pollsters
      interval: 300
      meters:
        - cpu
        - memory.usage
        - volume.size
```


## config/ceilometer/pipeline.yaml

### Pipeline

This configuration is strictly required for the Arcus self-service portal.

The `pipeline.yaml` file configures Ceilometer to the the custom metering policy
"`metering-policy`", a dependency of Arcus.

```
# config/ceilometer/pipeline.yaml
---
sources:
    - name: meter_source
      meters:
          - cpu
          - vcpus
          - memory
          - memory.usage
          - volume.size
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      publishers:
          - gnocchi://?archive_policy=metering-policy
```


## config/nova/nova-compute.conf

### Resource Reservations

To prevent OpenStack from starving your hosts of resources, you should set aside some. This is
particularly important in hyperconverged deployments.

```
# config/nova/nova-compute.conf

[DEFAULT]
reserved_host_memory_mb = 16384
reserved_host_cpus = 4
reserved_host_disk_mb = 5242880
```

### Overcommit ratios

We don't suggest overcommitting RAM, else you invite the OOM killer to your environment. We find
a 5:1 CPU overcommit ratio to be safe in production.

```
# config/nova/nova-compute.conf

[DEFAULT]
cpu_allocation_ratio = 5
ram_allocation_ratio = 1
```

## config/nova/nova-api.conf

### Disable Ephemral Disk Usage

Add these configs to prevent Openstack from using hypervisor's root disk.
It won't allow user to create instance using ephemeral disk.

```
# config/nova/nova-api.conf

[DEFAULT]
max_local_block_devices = 0
```


## config/neutron/ml2_conf.ini

### Enable VLANs

Neutron requires that the permitted VLAN range be defined on the physical provider network physnet1

```
#  config/neutron/ml2_conf.in

[ml2_type_vlan]
network_vlan_ranges = physnet1:1:4094
```


## config/neutron/dnsmasq.conf

### Network Associated Search Domain
Put entries to associate network with search domains along with default log-facility path.
Those networks will use search domains associated with them in this file instead of default
search domain.
```
# config/neutron/dnsmasq.conf
log-facility=/var/log/kolla/neutron/dnsmasq.log
domain=<search-domain-name-1>, <network-address-1>
.
.
.
domain=<search-domain-name-n>, <network-address-n>
```


## config/neutron/neutron-dhcp-agent.conf

### Default Search Domain
Add dns_name config in order to set deafult search domain. This search domain will be used by
all networks except ones that are defined in config/neutron/dnsmasq.conf.
```
# config/neutron/neutron-dhcp-agent.conf
# Don't forget to add "." at the end of serach domain name.
[DEFAULT]
dns_domain = <search-domain>.
```


## config/prometheus/prometheus.yml
Default scrape interval is 60 seconds. That means prometheus server will fetch monitoring data
from it's exporters after every one minute. It might put stress on server resources depending
on your servers specs. In order to use higher scrape interval we can configure
`prometheus.yml` before deployment. Replace values enclosed in `<>` with actual values.
Consult inventory file for checking what hosts belong to monitoring, control and networking.
All exporters are enabled by default. If any of the exporter is disabled in globals file,
don't put it's configs under `scrape_configs`. Elasticsearch exporter is enabled when elasticsearch
is enabled. Don't put it's configs under `scrape_configs` if elasticsearch isn't enabled.

```
# config/prometheus/prometheus.yml
global:
  scrape_interval: <scrape-interval>s
  scrape_timeout: 10s
  evaluation_interval: 15s
  external_labels:
    monitor: 'kolla'

scrape_configs:
  - job_name: prometheus
    static_configs:
      - targets:
        - '<monitoring host 1 api ip address>:9091'
        - '<monitoring host n api ip address>:9091'

  - job_name: node
    static_configs:
      - targets:
        # All openstack nodes participate in this job
        - '<openstack host 1 api ip address>:9100'
        - '<openstack host n api ip address>:9100'

  - job_name: mysqld
    static_configs:
      - targets:
        - '<control host 1 api ip address>:9104'
        - '<control host n api ip address>:9104'

  - job_name: haproxy
    static_configs:
      - targets:
        - '<network host 1 api ip address>:9101'
        - '<network host n api ip address>:9101'

  - job_name: memcached
    static_configs:
      - targets:
        - '<control host 1 api ip address>:9150'
        - '<control host n api ip address>:9150'

  - job_name: cadvisor
    static_configs:
      # All openstack nodes participate in this job
      - targets:
        - '<openstack host 1 api ip address>:18080'
        - '<openstack host n api ip address>:18080'


  - job_name: openstack_exporter
    honor_labels: true
    static_configs:
      - targets:
        - '<monitoring host 1 api ip address>:9198'
        - '<monitoring host n api ip address>:9198'

  - job_name: elasticsearch_exporter
    static_configs:
      - targets:
        - '<control host 1 api ip address>:9108'
        - '<control host n api ip address>:9108'

  - job_name: ceph_mgr_exporter
    # Add this job only if prometheus_ceph_mgr_exporter is enabled in globals file.
    honor_labels: true
    static_configs:
      - targets:
        - '<monitoring host 1 api ip address>:9283'
        - '<monitoring host n api ip address>:9283'

alerting:
  alertmanagers:
  - static_configs:
    - targets:
        - '<control host 1 api ip address>:9093'
        - '<control host n api ip address>:9093'
```

## config/keystone/keystone.conf

### Token Timeouts

Often our users upload HUGE glance images. Extending the token timeout helps with this.

```
# config/keystone/keystone.conf

[token]
expiration = 7200
```


### LDAP Configuration

Keystone can optionally use LDAP as an authentication backend. Breqwatr only supports LDAP using
Microsoft Active Directory.

- [OpenStack LDAP documentation](https://docs.openstack.org/keystone/pike/admin/identity-integrate-with-ldap.html)
- [Keystone config reference](https://docs.openstack.org/ocata/config-reference/identity/samples/keystone.conf.html)
- [Kolla-Ansible's keystone.conf Jinja2 template](https://github.com/openstack/kolla-ansible/blob/master/ansible/roles/keystone/templates/keystone.conf.j2)

By creating the domains directory, Kolla-Ansible's Jinja2 tempalte will automatically enable
`domain_specific_drivers_enabled`, so it doesn't need to set. The `[identity] driver` key still
needs to be defined.


## Keystone Domain-specific path: config/keystone/domains/keystone.DOMAIN\_NAME.conf

### Domain Creation

In the OpenStack cli, create the domain

```bash
openstack domain create <name>
```

### Domain Configuration

When using LDAP with Keystone, each domain's data should be configured in its own file.

Where the target domain to be "example.com", you would create a file named
`config/keystone/domains/keystone.example.com.conf` to define the domain details.


```ini
# config/keystone/domains/keystone.DOMAIN_NAME.conf

[identity]
driver = ldap


[ldap]

# Multiple LDAP URLs may be specified as a comma separated string
url = ldap://localhost

# Service account credentials
user = dc=Manager,dc=example,dc=org
password = samplepassword

# The default LDAP server suffix to use, if a DN is not defined via either
suffix = dc=example,dc=org

# Define the search trees for LDAP
user_tree_dn = ou=Users,dc=example,dc=org
group_tree_dn = ou=Groups,dc=example,dc=org

# Use a group to determine which users have access
#   including :1.2.840.113556.1.4.1941: enables nested AD group search
user_filter = (memberOf:1.2.840.113556.1.4.1941:=cn=grp-openstack,CN=Users,DC=ad,DC=local)
# If you included the nested search filter, also consider setting (default False):
group_ad_nesting = True

# Active Directory uses the following object classes & attributes
user_objectclass = person
group_objectclass = group
user_name_attribute = sAMAccountName
user_id_attribute = cn
user_mail_attribute = mail
user_pass_attribute = userPassword
user_enabled_attribute = userAccountControl
user_enabled_mask = 2
user_enabled_invert = false
user_enabled_default = 512
group_id_attribute = cn
group_name_attribute = ou
group_member_attribute = member
group_desc_attribute = description

# In big domains with nested OUs you may also need:
query_scope = sub
page_size = 0

# debug levels of 0,255, and 4095 are common, with 4095 the most verbose
debug_level = 0
```

